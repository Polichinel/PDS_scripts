{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some ideas\n",
    "\n",
    "- impliment and a number of different dicision tree and forest models and take the most important features. Then a feature could be the similarity between and other word... But you don't have words.. You have tweets... The sum of the simlilarity? The product?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HUSK:\n",
    "-recurrent nural networks for text\n",
    "-long short term memory\n",
    "- LSTM-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/polichinel/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/polichinel/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2698: DtypeWarning: Columns (0,1,4,7,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Basics -----------------------------------------------------\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Word2vec ----------------------------------------------------\n",
    "import gensim\n",
    "\n",
    "# NLTK -----------------------------------------------------\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "# Keras -----------------------------------------------\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkl_file = open('/media/polichinel/DATA/backup/PDS_DATA/time_lines.pkl', 'rb') # from feature set.\n",
    "\n",
    "# time_lines = pickle.load(pkl_file)\n",
    "\n",
    "# pkl_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unlabled_big = pd.read_csv(\"/media/polichinel/DATA/backup/PDS_DATA/new_many_tweets.csv\", index_col= 0)\n",
    "df_unlabled = pd.read_csv(\"all_unlabled_tweets.csv\", index_col= 0)\n",
    "df_labled = pd.read_csv(\"labled_tweets.csv\", index_col= 0)\n",
    "\n",
    "pkl_file = open('full_remove.pkl', 'rb') # from feature set.\n",
    "full_remove = pickle.load(pkl_file)\n",
    "pkl_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be enough - check before deleting the direct implimentation\n",
    "\n",
    "pkl_file = open('prep2.pkl', 'rb') # from feature set.\n",
    "prep2 = pickle.load(pkl_file)\n",
    "pkl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function from prerpossecing and feature selection\n",
    "\n",
    "# stop_words = []#[\"goptaxscam\"] # for now no stopwords\n",
    "\n",
    "# def prep2(text):\n",
    "#     wordlist = nltk.word_tokenize(text)\n",
    "#     wordlist = [lemmatizer.lemmatize(w.lower()) for w in wordlist]\n",
    "#     wordlist = [w for w in wordlist if w not in full_remove and w not in stop_words]\n",
    "#     return wordlist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_model(model, most_sim = \"president\"):\n",
    "    print(\"Most similar to {}:\\n\".format(most_sim))\n",
    "    for i in model.wv.most_similar(most_sim):\n",
    "        print(i)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    # get the Most common words\n",
    "    print(\"Most common words:\")\n",
    "    print(model.wv.index2word[0], model.wv.index2word[1], model.wv.index2word[2])\n",
    "\n",
    "    print(\"\\n\")\n",
    "    # get the least common words\n",
    "    vocab_size = len(model.wv.vocab)\n",
    "    print(\"Least common words:\")\n",
    "    print(model.wv.index2word[vocab_size - 1], model.wv.index2word[vocab_size - 2], model.wv.index2word[vocab_size - 3])\n",
    "\n",
    "    print(\"\\n\")\n",
    "    # some similarity fun\n",
    "    print(\"Sanity sim check1:\")\n",
    "    print(model.wv.similarity('woman', 'man'), model.wv.similarity('woman', 'thing'))\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"Sanity sim check2:\")\n",
    "    # what doesn't fit?\n",
    "    print(model.wv.doesnt_match(\"green blue red man\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14541"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tweets are back from 140 character limit; not 240!\n",
    "sentences = [prep2(sent) for sent in nltk.sent_tokenize(df_unlabled_big.text.to_string())]\n",
    "model = gensim.models.Word2Vec(sentences, iter=10, min_count=10, size=300,) # virker meget bedre med min_count = 10\n",
    "len(list(model.wv.vocab.keys())) # Men, med min_count = 10 har du kun 1650 obs... Flere tweets mere text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to president:\n",
      "\n",
      "('pres', 0.7580095529556274)\n",
      "('administration', 0.6565648317337036)\n",
      "('admin', 0.6336917877197266)\n",
      "('administratio', 0.5899003148078918)\n",
      "('administra', 0.5730597972869873)\n",
      "('president-elect', 0.5677915811538696)\n",
      "('donald', 0.5584909915924072)\n",
      "('adminis', 0.5568463206291199)\n",
      "('melania', 0.5033824443817139)\n",
      "('administrat', 0.5032073259353638)\n",
      "\n",
      "\n",
      "Most common words:\n",
      "you with today\n",
      "Least common words:\n",
      "tcpalm saveanimals twcnewshv\n",
      "\n",
      "\n",
      "Sanity sim check1:\n",
      "0.2964361291622894 0.071852714108319\n",
      "\n",
      "\n",
      "Sanity sim check2:\n",
      "man\n"
     ]
    }
   ],
   "source": [
    "check_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1650"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tweets are back from 140 character limit; not 240!\n",
    "sentences = [prep2(sent) for sent in nltk.sent_tokenize(df_unlabled.text.to_string())]\n",
    "model = gensim.models.Word2Vec(sentences, iter=10, min_count=10, size=300,) # virker meget bedre med min_count = 10\n",
    "len(list(model.wv.vocab.keys())) # Men, med min_count = 10 har du kun 1650 obs... Flere tweets mere text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to president:\n",
      "\n",
      "('trump', 0.8823031187057495)\n",
      "('obama', 0.8519636392593384)\n",
      "('donald', 0.8507498502731323)\n",
      "('admin', 0.8118516802787781)\n",
      "('mr.', 0.7699730396270752)\n",
      "('order', 0.7660565972328186)\n",
      "('executive', 0.7547990083694458)\n",
      "('comey', 0.7371382713317871)\n",
      "('pres', 0.7263409495353699)\n",
      "('decision', 0.7209031581878662)\n",
      "\n",
      "\n",
      "Most common words:\n",
      "this with you\n",
      "\n",
      "\n",
      "Least common words:\n",
      "hazleton thewilsontimes ppact\n",
      "\n",
      "\n",
      "Sanity sim check1:\n",
      "0.6238856500004997 0.4249420854335763\n",
      "\n",
      "\n",
      "Sanity sim check2:\n",
      "man\n"
     ]
    }
   ],
   "source": [
    "check_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = open('word2vec_small_model.pkl', 'wb')\n",
    "pickle.dump(model, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/media/polichinel/DATA/backup/PDS_DATA/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hot_comments_politics.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2009"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec_path = path + \"hot_comments_politics.pkl\"\n",
    "\n",
    "\n",
    "pkl_file = open(spec_path, 'rb') # from feature set.\n",
    "comments1 = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "\n",
    "# tweets are back from 140 character limit; not 240!\n",
    "sentences = [prep2(sent) for sent in nltk.sent_tokenize(pd.Series(comments1).to_string())]\n",
    "model = gensim.models.Word2Vec(sentences, iter=10, min_count=10, size=300,) # virker meget bedre med min_count = 10\n",
    "len(list(model.wv.vocab.keys())) # Men, med min_count = 10 har du kun 1650 obs... Flere tweets mere text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to president:\n",
      "\n",
      "('donald', 0.925031304359436)\n",
      "('team', 0.8672990202903748)\n",
      "('trump', 0.8623854517936707)\n",
      "('lie', 0.8352077603340149)\n",
      "('mueller', 0.8339289426803589)\n",
      "('mr.', 0.8317859768867493)\n",
      "('twitter', 0.8253746032714844)\n",
      "('fbi', 0.820846438407898)\n",
      "('asked', 0.8192439079284668)\n",
      "('trey', 0.8184199333190918)\n",
      "\n",
      "\n",
      "Most common words:\n",
      "that you this\n",
      "Least common words:\n",
      "biden agency parson\n",
      "\n",
      "\n",
      "Sanity sim check1:\n",
      "0.7952203822225943 0.5441836306365089\n",
      "\n",
      "\n",
      "Sanity sim check2:\n",
      "man\n"
     ]
    }
   ],
   "source": [
    "check_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### controversial_comments_politics.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4397"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec_path = path + \"controversial_comments_politics.pkl\"\n",
    "\n",
    "pkl_file = open(spec_path, 'rb') # from feature set.\n",
    "comments2 = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "\n",
    "# tweets are back from 140 character limit; not 240!\n",
    "sentences = [prep2(sent) for sent in nltk.sent_tokenize(pd.Series(comments2).to_string())]\n",
    "model = gensim.models.Word2Vec(sentences, iter=10, min_count=10, size=300,) # virker meget bedre med min_count = 10\n",
    "len(list(model.wv.vocab.keys())) # Men, med min_count = 10 har du kun 1650 obs... Flere tweets mere text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to president:\n",
      "\n",
      "('office', 0.6728357076644897)\n",
      "('biden', 0.6583060026168823)\n",
      "('senate', 0.647216796875)\n",
      "('running', 0.640929102897644)\n",
      "('2012', 0.6288430690765381)\n",
      "('kaine', 0.6215243339538574)\n",
      "('win', 0.6211662888526917)\n",
      "('2016', 0.602423906326294)\n",
      "('held', 0.583528995513916)\n",
      "('won', 0.5827171802520752)\n",
      "\n",
      "\n",
      "Most common words:\n",
      "you that n't\n",
      "Least common words:\n",
      "h-1b infantry pyramid\n",
      "\n",
      "\n",
      "Sanity sim check1:\n",
      "0.40652407702320764 0.25302125395193126\n",
      "\n",
      "\n",
      "Sanity sim check2:\n",
      "man\n"
     ]
    }
   ],
   "source": [
    "check_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hot_comments_news.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "913"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec_path = path + \"hot_comments_news.pkl\"\n",
    "\n",
    "pkl_file = open(spec_path, 'rb') # from feature set.\n",
    "comments3 = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "\n",
    "# tweets are back from 140 character limit; not 240!\n",
    "sentences = [prep2(sent) for sent in nltk.sent_tokenize(pd.Series(comments3).to_string())]\n",
    "model = gensim.models.Word2Vec(sentences, iter=10, min_count=10, size=300,) # virker meget bedre med min_count = 10\n",
    "len(list(model.wv.vocab.keys())) # Men, med min_count = 10 har du kun 1650 obs... Flere tweets mere text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to president:\n",
      "\n",
      "('nah', 0.9998939037322998)\n",
      "('under', 0.9998775124549866)\n",
      "('tv', 0.9998754262924194)\n",
      "('blame', 0.9998726844787598)\n",
      "('*', 0.9998718500137329)\n",
      "('yep', 0.999871015548706)\n",
      "('conservative', 0.9998701810836792)\n",
      "('able', 0.9998698830604553)\n",
      "('depends', 0.9998698234558105)\n",
      "('victim', 0.9998686909675598)\n",
      "\n",
      "\n",
      "Most common words:\n",
      "you that n't\n",
      "Least common words:\n",
      "dealer preacher bluehole\n",
      "\n",
      "\n",
      "Sanity sim check1:\n",
      "0.9992923430853986 0.9990753769188423\n",
      "\n",
      "\n",
      "Sanity sim check2:\n",
      "red\n"
     ]
    }
   ],
   "source": [
    "check_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### controversial_comments_news.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3174"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec_path = path + \"controversial_comments_news.pkl\"\n",
    "\n",
    "pkl_file = open(spec_path, 'rb') # from feature set.\n",
    "comments4 = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "\n",
    "# tweets are back from 140 character limit; not 240!\n",
    "sentences = [prep2(sent) for sent in nltk.sent_tokenize(pd.Series(comments4).to_string())]\n",
    "model = gensim.models.Word2Vec(sentences, iter=10, min_count=10, size=300,) # virker meget bedre med min_count = 10\n",
    "len(list(model.wv.vocab.keys())) # Men, med min_count = 10 har du kun 1650 obs... Flere tweets mere text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to president:\n",
      "\n",
      "('murderer', 0.7612804770469666)\n",
      "('rape', 0.7573052644729614)\n",
      "('allegedly', 0.7550442814826965)\n",
      "('privilege', 0.7535372972488403)\n",
      "('kkk', 0.7526537775993347)\n",
      "('disabled', 0.7520650625228882)\n",
      "('blair', 0.7508727312088013)\n",
      "('threw', 0.7443523406982422)\n",
      "('hillary', 0.739559531211853)\n",
      "('cps', 0.7373896837234497)\n",
      "\n",
      "\n",
      "Most common words:\n",
      "you that n't\n",
      "Least common words:\n",
      "dode blair tannerite\n",
      "\n",
      "\n",
      "Sanity sim check1:\n",
      "0.2890056112457652 0.06992142282022219\n",
      "\n",
      "\n",
      "Sanity sim check2:\n",
      "man\n"
     ]
    }
   ],
   "source": [
    "check_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Political top comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10107"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec_path = path + \"top_comments_politics.pkl\"\n",
    "\n",
    "\n",
    "pkl_file = open(spec_path, 'rb') # from feature set.\n",
    "comments5 = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "\n",
    "# tweets are back from 140 character limit; not 240!\n",
    "sentences = [prep2(sent) for sent in nltk.sent_tokenize(pd.Series(comments5).to_string())]\n",
    "model = gensim.models.Word2Vec(sentences, iter=10, min_count=10, size=300,) # virker meget bedre med min_count = 10\n",
    "len(list(model.wv.vocab.keys())) # Men, med min_count = 10 har du kun 1650 obs... Flere tweets mere text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to president:\n",
      "\n",
      "('pres', 0.6829893589019775)\n",
      "('potus', 0.6617478132247925)\n",
      "('presi', 0.6570675373077393)\n",
      "('rogers', 0.49445390701293945)\n",
      "('mrs.', 0.44786256551742554)\n",
      "('pardon', 0.4370495676994324)\n",
      "('nation', 0.41660720109939575)\n",
      "('arpaio', 0.41598495841026306)\n",
      "('donald', 0.4148949980735779)\n",
      "('press', 0.40888404846191406)\n",
      "\n",
      "\n",
      "Most common words:\n",
      "that this you\n",
      "Least common words:\n",
      "thermostat longmont equity\n",
      "\n",
      "\n",
      "Sanity sim check1:\n",
      "0.3030737060984742 0.013474127885121638\n",
      "\n",
      "\n",
      "Sanity sim check2:\n",
      "man\n"
     ]
    }
   ],
   "source": [
    "check_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News top comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10723"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec_path = path + \"top_comments_news.pkl\"\n",
    "\n",
    "\n",
    "pkl_file = open(spec_path, 'rb') # from feature set.\n",
    "comments6 = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "\n",
    "# tweets are back from 140 character limit; not 240!\n",
    "sentences = [prep2(sent) for sent in nltk.sent_tokenize(pd.Series(comments6).to_string())]\n",
    "model = gensim.models.Word2Vec(sentences, iter=10, min_count=10, size=300,) # virker meget bedre med min_count = 10\n",
    "len(list(model.wv.vocab.keys())) # Men, med min_count = 10 har du kun 1650 obs... Flere tweets mere text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to president:\n",
      "\n",
      "('donald', 0.6947949528694153)\n",
      "('obama', 0.6922512054443359)\n",
      "('erdogan', 0.6813598275184631)\n",
      "('election', 0.6658200025558472)\n",
      "('flynn', 0.6654534339904785)\n",
      "('trump', 0.6586183905601501)\n",
      "('administration', 0.6549455523490906)\n",
      "('mueller', 0.6484150886535645)\n",
      "('presidential', 0.636324942111969)\n",
      "('candidate', 0.6272989511489868)\n",
      "\n",
      "\n",
      "Most common words:\n",
      "that you this\n",
      "\n",
      "\n",
      "Least common words:\n",
      "psu catman wham\n",
      "\n",
      "\n",
      "Sanity sim check1:\n",
      "0.28168252810145206 0.0398605002457342\n",
      "\n",
      "\n",
      "Sanity sim check2:\n",
      "man\n"
     ]
    }
   ],
   "source": [
    "check_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregated Reddit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20130"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_all = comments1 + comments2 + comments3 + comments4 + comments5 + comments6\n",
    "\n",
    "\n",
    "# tweets are back from 140 character limit; not 240!\n",
    "sentences = [prep2(sent) for sent in nltk.sent_tokenize(pd.Series(comments_all).to_string())]\n",
    "model = gensim.models.Word2Vec(sentences, iter=10, min_count=10, size=300,) # virker meget bedre med min_count = 10\n",
    "len(list(model.wv.vocab.keys())) # Men, med min_count = 10 har du kun 1650 obs... Flere tweets mere text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to president:\n",
      "\n",
      "('presi', 0.706849217414856)\n",
      "('versa', 0.6527256965637207)\n",
      "('pres', 0.6406002640724182)\n",
      "('potus', 0.6244686841964722)\n",
      "('trump', 0.4279245436191559)\n",
      "('administ', 0.42070257663726807)\n",
      "('elizondo', 0.4203173518180847)\n",
      "('presidential', 0.41722384095191956)\n",
      "('impeached', 0.41068795323371887)\n",
      "('nominates', 0.406166672706604)\n",
      "\n",
      "\n",
      "Most common words:\n",
      "that you this\n",
      "\n",
      "\n",
      "Least common words:\n",
      "psu catman wham\n",
      "\n",
      "\n",
      "Sanity sim check1:\n",
      "0.26946671365302466 0.07333925890338251\n",
      "\n",
      "\n",
      "Sanity sim check2:\n",
      "man\n"
     ]
    }
   ],
   "source": [
    "check_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Aggrigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = comments_all + list(df_unlabled_big.text) + list(df_unlabled.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1947563"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30339"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this time we set a bit more options:\n",
    "\n",
    "# Set values for various parameters\n",
    "feature_size = 300    # Word vector dimensionality  \n",
    "window_context = 30          # Context window size                                                                                    \n",
    "min_word_count = 10   # Minimum word count                        \n",
    "sample = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "\n",
    "# tweets are back from 140 character limit; not 240!\n",
    "sentences = [prep2(sent) for sent in nltk.sent_tokenize(pd.Series(all_text).to_string())]\n",
    "model = gensim.models.Word2Vec(sentences, size=feature_size, window=window_context, min_count=min_word_count, sample=sample, iter=50)\n",
    "                               \n",
    "len(list(model.wv.vocab.keys())) # Men, med min_count = 10 har du kun 1650 obs... Flere tweets mere text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to president:\n",
      "\n",
      "('administration', 0.366122841835022)\n",
      "('potus', 0.35703083872795105)\n",
      "('trump', 0.3473072052001953)\n",
      "('pres', 0.33158835768699646)\n",
      "('admin', 0.3009606599807739)\n",
      "('donald', 0.2551809549331665)\n",
      "('obama', 0.23709678649902344)\n",
      "('appointee', 0.23350417613983154)\n",
      "('presi', 0.23258009552955627)\n",
      "('congress', 0.23108923435211182)\n",
      "\n",
      "\n",
      "Most common words:\n",
      "that you this\n",
      "\n",
      "\n",
      "Least common words:\n",
      "peterwsj bkesling usafmatthew\n",
      "\n",
      "\n",
      "Sanity sim check1:\n",
      "0.04791706610524503 -0.060643932791260444\n",
      "\n",
      "\n",
      "Sanity sim check2:\n",
      "man\n"
     ]
    }
   ],
   "source": [
    "check_model(model, \"president\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickling model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = open('word2vec_model.pkl', 'wb')\n",
    "pickle.dump(model, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.word_vec(\"potus\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROD!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    \n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model[word])\n",
    "    \n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "        \n",
    "    return(feature_vector)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return(np.array(features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get document level embeddings\n",
    "w2v_feature_array = averaged_word_vectorizer(corpus=tokenized_corpus, model=w2v_model,\n",
    "                                             num_features=feature_size)\n",
    "pd.DataFrame(w2v_feature_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
